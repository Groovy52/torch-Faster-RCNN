{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/harshatejas/pytorch_custom_object_detection/blob/main/Training.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "dW2onzHuXF2-"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/geun_19/anaconda3/envs/torch/lib/python3.7/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "# Imports\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os, re, cv2, pydicom, warnings\n",
        "\n",
        "from glob import glob\n",
        "from PIL import Image\n",
        "from matplotlib import pyplot as plt\n",
        "from pydicom.pixel_data_handlers.util import apply_voi_lut\n",
        "\n",
        "import torch\n",
        "import torchvision\n",
        "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
        "\n",
        "from engine import train_one_epoch, evaluate\n",
        "import utils\n",
        "import transforms as T\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "import albumentations as A\n",
        "from albumentations.pytorch.transforms import ToTensorV2\n",
        "from albumentations.core.transforms_interface import ImageOnlyTransform\n",
        "# from torchvision.models.detection import FasterRCNN\n",
        "from torchvision.models.detection.rpn import AnchorGenerator\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torch.utils.data.sampler import SequentialSampler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "5eRJbdSCXMAc"
      },
      "outputs": [],
      "source": [
        "# Hyperparameters\n",
        "# test_set_length = 40 \t\t # Test set (number of images)\n",
        "val_set_length = 3000\n",
        "train_batch_size = 16  \t\t # Train batch size\n",
        "# test_batch_size = 16    \t\t # Test batch size\n",
        "val_batch_size = 16   \n",
        "num_classes = 14+1        \t\t # Number of classes\n",
        "learning_rate = 0.005  \t\t # Learning rate\n",
        "num_epochs = 100    \t     # Number of epochs\n",
        "output_dir = \"/data1/geun_19/G-ff/py-faster_rcnn/weight/\"   # Output directory to save the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "base_dir = \"/data1/geun_19/G-ff/py-faster_rcnn/data/\"\n",
        "train_dicom_dir = base_dir + \"train/\"\n",
        "test_dicom_dir =  base_dir + \"test/\"\n",
        "val_dicom_dir =  base_dir + \"val/\"\n",
        "\n",
        "train_df_dir = base_dir +\"f_train.csv\"\n",
        "valid_df_dir = base_dir +\"f_val.csv\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_df = pd.read_csv(train_df_dir)\n",
        "valid_df = pd.read_csv(valid_df_dir)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "labels = train_df['class_name'].unique()\n",
        "labels_dict = {}\n",
        "for index, label in enumerate(labels):\n",
        "\tlabels_dict.__setitem__(index, label)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{0: 'No finding',\n",
              " 1: 'Cardiomegaly',\n",
              " 2: 'Aortic enlargement',\n",
              " 3: 'Pleural thickening',\n",
              " 4: 'ILD',\n",
              " 5: 'Pulmonary fibrosis',\n",
              " 6: 'Lung Opacity',\n",
              " 7: 'Atelectasis',\n",
              " 8: 'Other lesion',\n",
              " 9: 'Infiltration',\n",
              " 10: 'Nodule/Mass',\n",
              " 11: 'Pleural effusion',\n",
              " 12: 'Consolidation',\n",
              " 13: 'Calcification',\n",
              " 14: 'Pneumothorax'}"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "labels_dict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "Qq4RUifxXbP5"
      },
      "outputs": [],
      "source": [
        "# Helper functions \n",
        "def create_label_txt(path_to_csv):\n",
        "\n",
        "\tdata = pd.read_csv(path_to_csv)\n",
        "\tlabels = data['class_name'].unique()\n",
        "\n",
        "\tlabels_dict = {}\n",
        "\n",
        "\t# Creat dictionary from array\n",
        "\tfor index, label in enumerate(labels):\n",
        "\t\tlabels_dict.__setitem__(index, label)\n",
        "\t\n",
        "\n",
        "\t# We need to create labels.txt and write labels dictionary into it\n",
        "\twith open('/data1/geun_19/pytorch_custom_object_detection/data/labels.txt', 'w') as f:\n",
        "\t\tf.write(str(labels_dict))\n",
        "\n",
        "\treturn labels_dict\t\n",
        "\n",
        "def parse_one_annot(path, filename, labels_dict):\n",
        "\n",
        "\tdata = pd.read_csv(path)\n",
        "\n",
        "\tclass_names = data['class_name'].unique()\n",
        "\t# classes_df = data[data[\"filename \"] == filename][\"class\"]\n",
        "\tclasses_df = data[data[\"image_id\"]+\".dicom\" == filename][\"class_name\"]\n",
        " \n",
        "\tclasses_array = classes_df.to_numpy()\n",
        "\t\n",
        "\t# boxes_df = data[data[\"filename\"] == filename][[\"xmin\", \"ymin\", \"xmax\", \"ymax\"]]\n",
        "\tboxes_df = data[data[\"image_id\"]+\".dicom\" == filename][[\"x_min\", \"y_min\", \"x_max\", \"y_max\"]]\n",
        " \n",
        "\tboxes_array = boxes_df.to_numpy()\n",
        "\t\n",
        "\tclasses = []\n",
        "\tfor key, value in labels_dict.items():\n",
        "\t\tfor i in classes_array:\n",
        "\t\t\tif i == value:\n",
        "\t\t\t\tclasses.append(key)\n",
        "\n",
        "\t# Convert list to tuple\n",
        "\tclasses = tuple(classes)\n",
        "\n",
        "\treturn boxes_array, classes\n",
        "\n",
        "def get_model(num_classes):\n",
        "\n",
        "\t# Load an pre-trained object detectin model (in this case faster-rcnn)\n",
        "\tmodel = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained = True)\n",
        "\n",
        "\t# Number of input features\n",
        "\tin_features = model.roi_heads.box_predictor.cls_score.in_features\n",
        "\n",
        "\t# Replace the pre-trained head with a new head\n",
        "\tmodel.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
        "\n",
        "\treturn model\n",
        "\n",
        "\n",
        "def get_transforms(train):\n",
        "\n",
        "\ttransforms = []\n",
        "\n",
        "\t# Convert numpy image to PyTorch Tensor\n",
        "\ttransforms.append(T.ToTensor())\n",
        "\n",
        "\tif train:\n",
        "\t\t# Data augmentation\n",
        "\t\ttransforms.append(T.RandomHorizontalFlip(0.5))\n",
        "\n",
        "\treturn T.Compose(transforms) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "jCP5U5TiXqCh"
      },
      "outputs": [],
      "source": [
        "# class CardsDataset(torch.utils.data.Dataset):\n",
        "class BigDataset(torch.utils.data.Dataset):\n",
        "\n",
        "\t\"\"\" The dataset contains images of playing cards \n",
        "\t\tThe dataset includes images of king, queen, jack, ten, nine and ace playing cards\"\"\"\n",
        "\n",
        "\tdef __init__(self, dataset_dir, csv_file, labels_dict, transforms = None):\n",
        "\t\t# dataset_dir = dicom_dir\n",
        "\t\tself.dataset_dir = dataset_dir\n",
        "\t\tself.csv_file = csv_file\n",
        "\t\tself.transforms = transforms\n",
        "\t\tself.labels_dict = labels_dict\n",
        "\t\t# self.image_names = [file for file in sorted(os.listdir(os.path.join(dataset_dir))) if file.endswith('.jpg') or file.endswith('.JPG')]\n",
        "\t\tself.image_names = [file for file in sorted(os.listdir(os.path.join(dataset_dir))) if file.endswith('.dicom')]\n",
        "\n",
        "\tdef __getitem__(self, index):\n",
        "\n",
        "\t\timage_path = os.path.join(self.dataset_dir, self.image_names[index])\n",
        "\t\t# image = cv2.imread(image_path)\n",
        "\n",
        "\t\tdicom = pydicom.dcmread(image_path)\n",
        "\t\t# Convert BGR to RGB\n",
        "\t\t# image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "\t\timage = dicom.pixel_array\n",
        "\n",
        "\t\tbox_array, classes = parse_one_annot(self.csv_file, self.image_names[index], self.labels_dict)\n",
        "\t\tboxes = torch.as_tensor(box_array, dtype = torch.float32)\n",
        "\n",
        "\t\tlabels = torch.tensor(classes, dtype=torch.int64)\n",
        "\t\t\n",
        "\t\timage_id = torch.tensor([index])\n",
        "\t\tarea = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n",
        "\n",
        "\t\tiscrowd = torch.tensor(classes, dtype=torch.int64)\n",
        "\t\ttarget = {}\n",
        "\t\ttarget[\"boxes\"] = boxes\n",
        "\t\ttarget[\"labels\"] = labels\n",
        "\t\ttarget[\"image_id\"] = image_id\n",
        "\t\ttarget[\"area\"] = area\n",
        "\t\ttarget[\"iscrowd\"] = iscrowd\n",
        "\n",
        "\t\tif self.transforms is not None:\n",
        "\t\t\timage, target = self.transforms(image, target)\n",
        "\n",
        "\t\treturn image, target\n",
        "\n",
        "\tdef __len__(self):\n",
        "\n",
        "\t\treturn len(self.image_names)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'/data1/geun_19/G-ff/py-faster_rcnn/data/f_train.csv'"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_df_dir"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'/data1/geun_19/G-ff/py-faster_rcnn/data/train/'"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_dicom_dir"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "class VinBigDataset(Dataset): #Class to load Training Data\n",
        "    \n",
        "    def __init__(self, dataframe, image_dir, transforms=None,stat = 'Train'):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.image_ids = dataframe[\"image_id\"].unique()\n",
        "        self.df = dataframe\n",
        "        self.image_dir = image_dir\n",
        "        self.transforms = transforms\n",
        "        self.stat = stat\n",
        "        \n",
        "    def __getitem__(self, index):\n",
        "        if self.stat == 'Train':\n",
        "            \n",
        "            image_id = self.image_ids[index]\n",
        "            records = self.df[(self.df['image_id'] == image_id)]\n",
        "            records = records.reset_index(drop=True)\n",
        "\n",
        "            dicom = pydicom.dcmread(f\"{self.image_dir}/{image_id}.dicom\")\n",
        "\n",
        "            image = dicom.pixel_array\n",
        "            \n",
        "            if \"PhotometricInterpretation\" in dicom:\n",
        "                if dicom.PhotometricInterpretation == \"MONOCHROME1\":\n",
        "                    image = np.amax(image) - image\n",
        "\n",
        "            intercept = dicom.RescaleIntercept if \"RescaleIntercept\" in dicom else 0.0\n",
        "            slope = dicom.RescaleSlope if \"RescaleSlope\" in dicom else 1.0\n",
        "\n",
        "            if slope != 1:\n",
        "                image = slope * image.astype(np.float64)\n",
        "                image = image.astype(np.int16)\n",
        "                \n",
        "            image += np.int16(intercept)        \n",
        "\n",
        "            image = np.stack([image, image, image])\n",
        "            image = image.astype('float32')\n",
        "            image = image - image.min()\n",
        "            image = image / image.max()\n",
        "            image = image * 255.0\n",
        "            image = image.transpose(1,2,0)\n",
        "\n",
        "            if records.loc[0, \"class_id\"] == 0:\n",
        "                records = records.loc[[0], :]\n",
        "                \n",
        "            boxes = records[['x_min', 'y_min', 'x_max', 'y_max']].values\n",
        "            area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n",
        "            area = torch.as_tensor(area, dtype=torch.float32)\n",
        "            labels = torch.tensor(records[\"class_id\"].values, dtype=torch.int64)\n",
        "\n",
        "            # suppose all instances are not crowd\n",
        "            iscrowd = torch.zeros((records.shape[0],), dtype=torch.int64)\n",
        "\n",
        "            target = {}\n",
        "            target['boxes'] = boxes\n",
        "            target['labels'] = labels\n",
        "            target['image_id'] = torch.tensor([index])\n",
        "            target['area'] = area\n",
        "            target['iscrowd'] = iscrowd\n",
        "            \n",
        "            if self.transforms:\n",
        "                sample = {\n",
        "                    'image': image,\n",
        "                    'bboxes': target['boxes'],\n",
        "                    'labels': labels\n",
        "                }\n",
        "                sample = self.transforms(**sample)\n",
        "                image = sample['image']\n",
        "\n",
        "                target['boxes'] = torch.tensor(sample['bboxes'])\n",
        "\n",
        "            if target[\"boxes\"].shape[0] == 0:\n",
        "                # Albumentation cuts the target (class 14, 1x1px in the corner)\n",
        "                target[\"boxes\"] = torch.from_numpy(np.array([[0.0, 0.0, 1.0, 1.0]]))\n",
        "                target[\"area\"] = torch.tensor([1.0], dtype=torch.float32)\n",
        "                target[\"labels\"] = torch.tensor([0], dtype=torch.int64)\n",
        "\n",
        "            # return image, target, image_ids\n",
        "            return image, target\n",
        "        \n",
        "        else:\n",
        "                   \n",
        "            image_id = self.image_ids[index]\n",
        "            records = self.df[(self.df['image_id'] == image_id)]\n",
        "            records = records.reset_index(drop=True)\n",
        "\n",
        "            dicom = pydicom.dcmread(f\"{self.image_dir}/{image_id}.dicom\")\n",
        "\n",
        "            image = dicom.pixel_array\n",
        "\n",
        "            intercept = dicom.RescaleIntercept if \"RescaleIntercept\" in dicom else 0.0\n",
        "            slope = dicom.RescaleSlope if \"RescaleSlope\" in dicom else 1.0\n",
        "\n",
        "            if slope != 1:\n",
        "                image = slope * image.astype(np.float64)\n",
        "                image = image.astype(np.int16)\n",
        "\n",
        "            image += np.int16(intercept)\n",
        "            \n",
        "            image = np.stack([image, image, image])\n",
        "            image = image.astype('float32')\n",
        "            image = image - image.min()\n",
        "            image = image / image.max()\n",
        "            image = image * 255.0\n",
        "            image = image.transpose(1,2,0)\n",
        "\n",
        "            if self.transforms:\n",
        "                sample = {\n",
        "                    'image': image,\n",
        "                }\n",
        "                sample = self.transforms(**sample)\n",
        "                image = sample['image']\n",
        "\n",
        "            # return image, image_id\n",
        "            return image\n",
        "    \n",
        "    def __len__(self):\n",
        "        return self.image_ids.shape[0]\n",
        "\n",
        "def dilation(img): # custom image processing function\n",
        "    kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, tuple(np.random.randint(1, 6, 2)))\n",
        "    img = cv2.dilate(img, kernel, iterations=1)\n",
        "    return img\n",
        "\n",
        "class Dilation(ImageOnlyTransform):\n",
        "    def apply(self, img, **params):\n",
        "        return dilation(img)   \n",
        "    \n",
        "def get_train_transform():\n",
        "    return A.Compose([\n",
        "        A.Flip(0.5),\n",
        "        A.ShiftScaleRotate(scale_limit=0.1, rotate_limit=45, p=0.25),\n",
        "        A.LongestMaxSize(max_size=800, p=1.0),\n",
        "        Dilation(),\n",
        "        # FasterRCNN will normalize.\n",
        "        A.Normalize(mean=(0, 0, 0), std=(1, 1, 1), max_pixel_value=255.0, p=1.0),\n",
        "        ToTensorV2(p=1.0)\n",
        "    ], bbox_params={'format': 'pascal_voc', 'label_fields': ['labels']})\n",
        "    \n",
        "def get_valid_transform():\n",
        "    return A.Compose([\n",
        "        A.Normalize(mean=(0, 0, 0), std=(1, 1, 1), max_pixel_value=255.0, p=1.0),\n",
        "        ToTensorV2(p=1.0)\n",
        "    ], bbox_params={'format': 'pascal_voc', 'label_fields': ['labels']})\n",
        "\n",
        "def get_test_transform():\n",
        "    return A.Compose([\n",
        "        A.Normalize(mean=(0, 0, 0), std=(1, 1, 1), max_pixel_value=255.0, p=1.0),\n",
        "        ToTensorV2(p=1.0)\n",
        "    ])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "num_workers = 4 * torch.cuda.device_count()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Setting up the device\n",
        "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "x = torch.tensor([1., 2.]).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qAdrwUO4Xs0j",
        "outputId": "0beedec2-c48b-4d08-ca2d-c7714f9035cb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "We have: 15000 images in the dataset, 12000 are training images and 3000 are validation images\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# labels_dict = create_label_txt(\"cards_dataset/train_labels.csv\")\n",
        "labels_dict = create_label_txt(train_df_dir)\n",
        "\n",
        "# Define train and test dataset\n",
        "# dataset = CardsDataset(dataset_dir = \"cards_dataset/train/\", csv_file = \"cards_dataset/train_labels.csv\",\n",
        "#                         labels_dict = labels_dict, transforms = get_transforms(train = True))\n",
        "# dataset = VinBigDataset(dataset_dir = train_dicom_dir, csv_file = train_df_dir,\n",
        "#                         labels_dict = labels_dict, transforms = get_transforms(train = True))\n",
        "\n",
        "# dataset_test = CardsDataset(dataset_dir = \"cards_dataset/train/\", csv_file = \"cards_dataset/train_labels.csv\", \n",
        "#                         labels_dict = labels_dict, transforms = get_transforms(train = False))\n",
        "# dataset_val = VinBigDataset(dataset_dir = val_dicom_dir, csv_file = valid_df_dir, \n",
        "#                         labels_dict = labels_dict, transforms = get_transforms(train = False))\n",
        "\n",
        "train_dataset = VinBigDataset(train_df, train_dicom_dir, get_train_transform())\n",
        "valid_dataset = VinBigDataset(valid_df, val_dicom_dir, get_valid_transform())\n",
        "\n",
        "# Split the dataset into train and test\n",
        "# torch.manual_seed(1)\n",
        "# indices = torch.randperm(len(dataset)).tolist()\n",
        "# dataset = torch.utils.data.Subset(dataset, indices[:-test_set_length])\n",
        "# dataset_test = torch.utils.data.Subset(dataset_test, indices[-test_set_length:])\n",
        "\n",
        "# Define train and test dataloaders\n",
        "# data_loader = torch.utils.data.DataLoader(dataset, batch_size = train_batch_size, shuffle = True,\n",
        "#                 num_workers = 2, collate_fn = utils.collate_fn)\n",
        "\n",
        "# data_loader_test = torch.utils.data.DataLoader(dataset_val, batch_size = val_batch_size, shuffle = False,\n",
        "#                 num_workers = 2, collate_fn = utils.collate_fn)\n",
        "train_data_loader = torch.utils.data.DataLoader(train_dataset, batch_size = train_batch_size, shuffle = True,\n",
        "                num_workers = 2, collate_fn = utils.collate_fn)\n",
        "\n",
        "valid_data_loader = torch.utils.data.DataLoader(valid_dataset, batch_size = val_batch_size, shuffle = False,\n",
        "                num_workers = 2, collate_fn = utils.collate_fn)\n",
        "\n",
        "# print(f\"We have: {len(indices)} images in the dataset, {len(dataset)} are training images and {len(dataset_test)} are test images\")\n",
        "print(f\"We have: {len(train_dataset)+len(valid_dataset)} images in the dataset, {len(train_dataset)} are training images and {len(valid_dataset)} are validation images\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<torch.utils.data.dataloader.DataLoader at 0x7f66f068fad0>"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_data_loader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<__main__.VinBigDataset at 0x7f66f068fdd0>"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'/data1/geun_19/G-ff/py-faster_rcnn/weight/'"
            ]
          },
          "execution_count": 44,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "output_dir"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "108e82fca7124528a142d16b972d9cf4",
            "423a059765df40fa8ff9b08a4190bcce",
            "7fdbee2cdbcf40909522e9178eb230a6",
            "43047a0323ab4ab19adc84a14c9d610e",
            "d13a194299014e3980cfa98d6bbffd1e",
            "b1612279a67a46a89903620336ca4632",
            "e1125572080541d3a82122efa25cf2ea",
            "e05937d44691445b8d2040d554bb1210"
          ]
        },
        "id": "ZGy4HnILab5Z",
        "outputId": "5f020524-d053-49ea-adb1-e510bd8b6172"
      },
      "outputs": [
        {
          "ename": "RuntimeError",
          "evalue": "CUDA out of memory. Tried to allocate 800.00 MiB (GPU 0; 11.91 GiB total capacity; 8.31 GiB already allocated; 735.00 MiB free; 10.57 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipykernel_18116/1666755389.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0mtrain_one_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_data_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprint_freq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m     \u001b[0mlr_scheduler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;31m# Evaluate on the test dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/data1/geun_19/pytorch_custom_object_detection/engine.py\u001b[0m in \u001b[0;36mtrain_one_epoch\u001b[0;34m(model, optimizer, data_loader, device, epoch, print_freq)\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0mtargets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m         \u001b[0mloss_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0mlosses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mloss_dict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/anaconda3/envs/torch/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/anaconda3/envs/torch/lib/python3.7/site-packages/torchvision/models/detection/generalized_rcnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, images, targets)\u001b[0m\n\u001b[1;32m     93\u001b[0m                     )\n\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m         \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackbone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m             \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOrderedDict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"0\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/anaconda3/envs/torch/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/anaconda3/envs/torch/lib/python3.7/site-packages/torchvision/models/detection/backbone_utils.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mDict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfpn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/anaconda3/envs/torch/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/anaconda3/envs/torch/lib/python3.7/site-packages/torchvision/ops/feature_pyramid_network.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    150\u001b[0m             \u001b[0mfeat_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minner_lateral\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m             \u001b[0minner_top_down\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minterpolate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlast_inner\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeat_shape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"nearest\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 152\u001b[0;31m             \u001b[0mlast_inner\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minner_lateral\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0minner_top_down\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    153\u001b[0m             \u001b[0mresults\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minsert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_result_from_layer_blocks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlast_inner\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 800.00 MiB (GPU 0; 11.91 GiB total capacity; 8.31 GiB already allocated; 735.00 MiB free; 10.57 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
          ]
        },
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
          ]
        }
      ],
      "source": [
        "# Get the model using helper function\n",
        "model = get_model(num_classes)\n",
        "model.to(device = device)\n",
        "\n",
        "# Construct the optimizer\n",
        "params = [p for p in model.parameters() if p.requires_grad]\n",
        "optimizer = torch.optim.SGD(params, lr = learning_rate, momentum = 0.9, weight_decay = 0.0005)\n",
        "\n",
        "# Learning rate scheduler decreases the learning rate by 10x every 3 epochs\n",
        "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size = 3, gamma = 0.1)\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    \n",
        "    train_one_epoch(model, optimizer, train_data_loader, device, epoch, print_freq = 10)\n",
        "    lr_scheduler.step()\n",
        "    # Evaluate on the test dataset\n",
        "    evaluate(model, valid_data_loader, device = device)\n",
        "\n",
        "if not os.path.exists(output_dir):\n",
        "    os.mkdir(output_dir)\n",
        "\n",
        "# Save the model state\t\n",
        "# torch.save(model.state_dict(), output_dir + \"/model\")\n",
        "torch.save(model.state_dict(), output_dir)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# new code\n",
        "\n",
        "https://www.kaggle.com/code/mariazorkaltseva/vinbigdata-eda-faster-rcnn-icevision-training/notebook"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [],
      "source": [
        "# import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as ticker\n",
        "\n",
        "import pytorch_lightning as pl\n",
        "from pytorch_lightning.loggers import CSVLogger\n",
        "\n",
        "from sklearn.model_selection import GroupKFold\n",
        "# from icevision.all import *\n",
        "\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [],
      "source": [
        "import random"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [],
      "source": [
        "SEED = 2021\n",
        "DEBUG = False\n",
        "IMG_DIM = 512\n",
        "RESIZE_DIM = 384\n",
        "PRESIZE = 512\n",
        "BATCH_SIZE = 16 # 48\n",
        "NUM_WORKERS = 4\n",
        "N_FOLDS = 5\n",
        "FOLDS_IDS = [0]\n",
        "LR = 1e-5\n",
        "WDECAY = 1e-4\n",
        "NUM_EPOCHS = 60\n",
        "\n",
        "def seed_everything(seed):\n",
        "    random.seed(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = True\n",
        "\n",
        "seed_everything(SEED)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2022-05-11 15:42:36--  https://raw.githubusercontent.com/airctic/icevision/master/icevision_install.sh\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.110.133, 185.199.109.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2766 (2.7K) [text/plain]\n",
            "Saving to: ‘icevision_install.sh’\n",
            "\n",
            "icevision_install.s 100%[===================>]   2.70K  --.-KB/s    in 0s      \n",
            "\n",
            "2022-05-11 15:42:36 (22.3 MB/s) - ‘icevision_install.sh’ saved [2766/2766]\n",
            "\n",
            "Installing icevision + dependencices for cuda11\n",
            "- Installing torch and its dependencies\n",
            "Looking in links: https://download.pytorch.org/whl/torch_stable.html\n",
            "Collecting torch==1.10.0+cu111\n",
            "  Downloading https://download.pytorch.org/whl/cu111/torch-1.10.0%2Bcu111-cp37-cp37m-linux_x86_64.whl (2137.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 2137.6 MB 504 bytes/s a 0:00:01   |█▎                              | 82.7 MB 13.1 MB/s eta 0:02:38     |█████████████████▏              | 1146.2 MB 12.9 MB/s eta 0:01:17\n",
            "\u001b[?25hCollecting torchvision==0.11.1+cu111\n",
            "  Downloading https://download.pytorch.org/whl/cu111/torchvision-0.11.1%2Bcu111-cp37-cp37m-linux_x86_64.whl (24.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 24.5 MB 12.1 MB/s eta 0:00:01\n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /home/geun_19/anaconda3/envs/torch/lib/python3.7/site-packages (from torch==1.10.0+cu111) (4.1.1)\n",
            "Requirement already satisfied: pillow!=8.3.0,>=5.3.0 in /home/geun_19/anaconda3/envs/torch/lib/python3.7/site-packages (from torchvision==0.11.1+cu111) (8.4.0)\n",
            "Requirement already satisfied: numpy in /home/geun_19/anaconda3/envs/torch/lib/python3.7/site-packages (from torchvision==0.11.1+cu111) (1.21.5)\n",
            "Installing collected packages: torch, torchvision\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 1.10.2\n",
            "    Uninstalling torch-1.10.2:\n",
            "      Successfully uninstalled torch-1.10.2\n",
            "  Attempting uninstall: torchvision\n",
            "    Found existing installation: torchvision 0.11.3\n",
            "    Uninstalling torchvision-0.11.3:\n",
            "      Successfully uninstalled torchvision-0.11.3\n",
            "Successfully installed torch-1.10.0+cu111 torchvision-0.11.1+cu111\n",
            "- Installing mmcv\n",
            "- Installing mmdet\n",
            "- Installing mmseg\n",
            "- Installing icevision from PyPi\n",
            "\u001b[33mWARNING: Discarding https://files.pythonhosted.org/packages/e5/54/4f96c51b171cf3a64a04b8c5167268803205bc5943b5cdf70bd770727b88/oauthlib-1.1.0-1.tar.gz#sha256=0f786c5573248a38efa86c48c59c0c93140ac836ab2a246aeefd8f9039e999ba (from https://pypi.org/simple/oauthlib/). Requested oauthlib>=0.6.2 from https://files.pythonhosted.org/packages/e5/54/4f96c51b171cf3a64a04b8c5167268803205bc5943b5cdf70bd770727b88/oauthlib-1.1.0-1.tar.gz#sha256=0f786c5573248a38efa86c48c59c0c93140ac836ab2a246aeefd8f9039e999ba (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.2.0->pytorch-lightning>=1.4.5->icevision[all]) has inconsistent version: filename has '1.1.0.post1', but metadata has '1.1.0'\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "# Torch - Torchvision - IceVision - IceData - MMDetection - YOLOv5 - EfficientDet Installation\n",
        "!wget https://raw.githubusercontent.com/airctic/icevision/master/icevision_install.sh\n",
        "\n",
        "# Choose your installation target: cuda11 or cuda10 or cpu\n",
        "!bash icevision_install.sh cuda11"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'status': 'ok', 'restart': True}"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
          ]
        }
      ],
      "source": [
        "# Restart kernel after installation\n",
        "import IPython\n",
        "IPython.Application.instance().kernel.do_shutdown(True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class PL_Model(faster_rcnn.lightning.ModelAdapter):       \n",
        "    def configure_optimizers(self):\n",
        "        optimizer = torch.optim.AdamW(self.parameters(), lr=LR, weight_decay=WDECAY)\n",
        "        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, \n",
        "                                                               factor=0.1, \n",
        "                                                               mode='min', \n",
        "                                                               patience=10)\n",
        "        \n",
        "#         scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, \n",
        "#                                                                12,\n",
        "#                                                                eta_min=0.01, \n",
        "#                                                                last_epoch=-1)\n",
        "        return [optimizer], [{\"scheduler\": scheduler,\n",
        "                              \"interval\": 'epoch',\n",
        "                              'monitor': 'val_loss'}]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "authorship_tag": "ABX9TyPmaOsEJqJRerJlO66kMvdz",
      "collapsed_sections": [],
      "include_colab_link": true,
      "name": "Training.ipynb",
      "provenance": []
    },
    "interpreter": {
      "hash": "d1b98219f7ceb40c50796be30af8ed0139a552bcdca14b3e1e4cfa56a447f17c"
    },
    "kernelspec": {
      "display_name": "Python 3.7.13 ('torch')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.13"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "108e82fca7124528a142d16b972d9cf4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_7fdbee2cdbcf40909522e9178eb230a6",
              "IPY_MODEL_43047a0323ab4ab19adc84a14c9d610e"
            ],
            "layout": "IPY_MODEL_423a059765df40fa8ff9b08a4190bcce"
          }
        },
        "423a059765df40fa8ff9b08a4190bcce": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "43047a0323ab4ab19adc84a14c9d610e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e05937d44691445b8d2040d554bb1210",
            "placeholder": "​",
            "style": "IPY_MODEL_e1125572080541d3a82122efa25cf2ea",
            "value": " 160M/160M [00:46&lt;00:00, 3.58MB/s]"
          }
        },
        "7fdbee2cdbcf40909522e9178eb230a6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "100%",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b1612279a67a46a89903620336ca4632",
            "max": 167502836,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d13a194299014e3980cfa98d6bbffd1e",
            "value": 167502836
          }
        },
        "b1612279a67a46a89903620336ca4632": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d13a194299014e3980cfa98d6bbffd1e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": "initial"
          }
        },
        "e05937d44691445b8d2040d554bb1210": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e1125572080541d3a82122efa25cf2ea": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
